{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neverix/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 19 files: 100%|██████████| 19/19 [00:00<00:00, 76553.10it/s]\n",
      "The config attributes {'force_zeros_for_empty_prompt': True, 'add_watermarker': None} were passed to FlaxStableDiffusionXLPipeline, but are not expected and will be ignored. Please verify your model_index.json configuration file.\n",
      "Keyword arguments {'force_zeros_for_empty_prompt': True, 'add_watermarker': None} are not expected by FlaxStableDiffusionXLPipeline and will be ignored.\n",
      "The config attributes {'interpolation_type': 'linear', 'use_karras_sigmas': False} were passed to FlaxEulerDiscreteScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "The config attributes {'act_fn': 'silu', 'attention_type': 'default', 'center_input_sample': False, 'class_embed_type': None, 'class_embeddings_concat': False, 'conv_in_kernel': 3, 'conv_out_kernel': 3, 'cross_attention_norm': None, 'downsample_padding': 1, 'dual_cross_attention': False, 'encoder_hid_dim': None, 'encoder_hid_dim_type': None, 'mid_block_only_cross_attention': None, 'mid_block_scale_factor': 1, 'mid_block_type': 'UNetMidBlock2DCrossAttn', 'norm_eps': 1e-05, 'norm_num_groups': 32, 'num_class_embeds': None, 'resnet_out_scale_factor': 1.0, 'resnet_skip_time_act': False, 'resnet_time_scale_shift': 'default', 'reverse_transformer_layers_per_block': None, 'time_cond_proj_dim': None, 'time_embedding_act_fn': None, 'time_embedding_dim': None, 'time_embedding_type': 'positional', 'timestep_post_act': None, 'upcast_attention': None} were passed to FlaxUNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/huggingface/hub/models--nev--lcm-sdxl-flax/snapshots/52fb47f31f449167eb7ab744a2ff35e006ff8ec8/unet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'force_upcast': True} were passed to FlaxAutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/neverix/.cache/huggingface/hub/models--nev--lcm-sdxl-flax/snapshots/52fb47f31f449167eb7ab744a2ff35e006ff8ec8/vae\n"
     ]
    }
   ],
   "source": [
    "from diffusers import FlaxStableDiffusionXLPipeline\n",
    "pipe, params = FlaxStableDiffusionXLPipeline.from_pretrained(\n",
    "    \"nev/lcm-sdxl-flax\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "dtype = jnp.float32\n",
    "params = jax.tree_util.tree_map(lambda x: x.astype(dtype), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training.common_utils import shard\n",
    "from flax.jax_utils import replicate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "p_params = replicate(params)\n",
    "\n",
    "def create_key(seed=0):\n",
    "    return jax.random.PRNGKey(seed)\n",
    "\n",
    "rng = create_key(0)\n",
    "rng = jax.random.split(rng, jax.device_count())\n",
    "\n",
    "do_jit = True\n",
    "\n",
    "def generate(prompt_ids):\n",
    "    images = pipe(\n",
    "        prompt_ids if do_jit else prompt_ids[0],\n",
    "        p_params if do_jit else params,\n",
    "        rng if do_jit else rng[0],\n",
    "        num_inference_steps=1,\n",
    "        # neg_prompt_ids=neg_prompt_ids if do_jit else neg_prompt_ids[0],\n",
    "        guidance_scale = 1.0,\n",
    "        jit=do_jit,\n",
    "    ).images\n",
    "    images = images.reshape((images.shape[0] * images.shape[1], ) + images.shape[-3:])\n",
    "    return pipe.numpy_to_pil(np.array(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 15.54G of 15.48G hbm. Exceeded hbm capacity by 57.28M.\n\nTotal hbm usage >= 16.06G:\n    reserved        530.00M \n    program           2.61G \n    arguments        12.93G \n\nOutput size 12.00M; shares 0B with arguments.\n\nProgram hbm requirement 2.61G:\n    global            83.0K\n    scoped           161.0K\n    HLO temp          2.54G (100.0% utilization: Unpadded (2.03G) Padded (2.03G), 20.1% fragmentation (522.18M))\n    overlays         65.82M\n\n  Largest program allocations in hbm:\n\n  1. Size: 524.00M\n     Shape: bf16[1024,8,131,256]{3,0,2,1:T(8,128)(2,1)}\n     Unpadded size: 524.00M\n     XLA label: fusion.85.remat.1 = fusion(slice.509, bitcast.2878, bitcast.7047), kind=kLoop, calls=fused_computation.73.clone.1\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 524.00M\n     Shape: bf16[1024,8,131,256]{3,0,2,1:T(8,128)(2,1)}\n     Unpadded size: 524.00M\n     XLA label: fusion.85.remat.1 = fusion(slice.509, bitcast.2878, bitcast.7047), kind=kLoop, calls=fused_computation.73.clone.1\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 516.00M\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv_shortcut/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: f32[1024,8,129,128]{3,1,2,0:T(8,128)}\n     Unpadded size: 516.00M\n     XLA label: fusion.253.remat = fusion(get-tuple-element.2519, fusion.95.remat, constant.1828, get-tuple-element.2518), kind=kOutput, calls=fused_computation.186.clone\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 516.00M\n     Shape: bf16[1024,1,1032,256]{3,0,2,1:T(8,128)(2,1)}\n     Unpadded size: 516.00M\n     XLA label: fusion.18936 = fusion(copy.6580), kind=kLoop, calls=fused_computation.12772.clone\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 65.82M\n     XLA label: overlays\n     Allocation type: overlays\n     ==========================\n\n  6. Size: 17.0K\n     Shape: pred[8,129]{0,1:T(8,128)(4,1)}\n     Unpadded size: 1.0K\n     Extra memory due to padding: 16.0K (16.9x expansion)\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  7. Size: 9.0K\n     Shape: pred[8,65]{0,1:T(8,128)(4,1)}\n     Unpadded size: 520B\n     Extra memory due to padding: 8.5K (17.7x expansion)\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  8. Size: 5.0K\n     Shape: pred[8,33]{0,1:T(8,128)(4,1)}\n     Unpadded size: 264B\n     Extra memory due to padding: 4.7K (19.4x expansion)\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  9. Size: 3.0K\n     Shape: pred[8,17]{0,1:T(8,128)(4,1)}\n     Unpadded size: 136B\n     Extra memory due to padding: 2.9K (22.6x expansion)\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  10. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/while/body/FlaxUNet2DConditionModel/mid_block/attentions_0/transformer_blocks_2/attn2/to_out_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[2,1024]{1,0:T(2,128)}, f32[2,1024]{1,0:T(2,128)}, f32[2,1024,1280]{2,1,0:T(8,128)})\n     Unpadded size: 1.0K\n     XLA label: fusion.14368 = fusion(get-tuple-element.6536, get-tuple-element.3343, bitcast.1304, bitcast.9178), kind=kOutput, calls=fused_computation.12617\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/while/body/FlaxUNet2DConditionModel/mid_block/attentions_0/transformer_blocks_2/attn2/b i d, b j d->b i j/dot_general[dimension_numbers=(((2,), (2,)), ((0,), (0,))) precision=None preferred_element_type=float32]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[40,1024]{1,0:T(8,128)}, f32[40,1024,77]{1,2,0:T(8,128)})\n     Unpadded size: 1.0K\n     XLA label: fusion.15566 = fusion(bitcast.1293, bitcast.1297), kind=kOutput, calls=fused_computation.12897\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_2/norm1/div\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[32]{0:T(256)}, f32[32]{0:T(256)})\n     Unpadded size: 1.0K\n     XLA label: fusion.12878 = fusion(reduce.1300, reduce.1302), kind=kLoop, calls=fused_computation.11278\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv2/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[128]{0:T(256)}, f32[128]{0:T(256)}, f32[1024,8,129,128]{3,1,2,0:T(8,128)})\n     Unpadded size: 1.0K\n     XLA label: fusion.267 = fusion(constant.1828, get-tuple-element.6881, get-tuple-element.2526, get-tuple-element.2527, ...(+3)), kind=kOutput, calls=fused_computation.196\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 1.0K\n     Shape: (bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)}, bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)})\n     Unpadded size: 1.0K\n     XLA label: fusion.15362 = fusion(fusion.268), kind=kLoop, calls=fused_computation.12777\n     Allocation type: HLO temp\n     ==========================\n\n  15. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/norm2/div\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[32]{0:T(256)}, f32[32]{0:T(256)})\n     Unpadded size: 1.0K\n     XLA label: fusion.12879 = fusion(reduce.1297, reduce.1299), kind=kLoop, calls=fused_computation.11279\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[128]{0:T(256)}, f32[128]{0:T(256)}, f32[1024,8,129,128]{3,1,2,0:T(8,128)})\n     Unpadded size: 1.0K\n     XLA label: fusion.270 = fusion(constant.1828, get-tuple-element.2524, get-tuple-element.2525, get-tuple-element.5361, ...(+2)), kind=kOutput, calls=fused_computation.199\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 1.0K\n     Shape: (bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)}, bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)})\n     Unpadded size: 1.0K\n     XLA label: fusion.15363 = fusion(fusion.271), kind=kLoop, calls=fused_computation.12778\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/norm1/div\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[32]{0:T(256)}, f32[32]{0:T(256)})\n     Unpadded size: 1.0K\n     XLA label: fusion.12880 = fusion(reduce.1294, reduce.1296), kind=kLoop, calls=fused_computation.11280\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv2/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[128]{0:T(256)}, f32[128]{0:T(256)}, f32[1024,8,129,128]{3,1,2,0:T(8,128)})\n     Unpadded size: 1.0K\n     XLA label: fusion.273 = fusion(constant.1828, fusion.253.remat, get-tuple-element.2516, get-tuple-element.2517, ...(+3)), kind=kOutput, calls=fused_computation.202\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 1.0K\n     Shape: (bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)}, bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)})\n     Unpadded size: 1.0K\n     XLA label: fusion.15364 = fusion(fusion.274), kind=kLoop, calls=fused_computation.12779\n     Allocation type: HLO temp\n     ==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/neverix/misgen/experiment_lcm.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m prompt_ids \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39mprepare_inputs(prompts)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m prompt_ids \u001b[39m=\u001b[39m shard(prompt_ids)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m u \u001b[39min\u001b[39;00m generate(prompt_ids):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     display(u)\n",
      "\u001b[1;32m/home/neverix/misgen/experiment_lcm.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(prompt_ids):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     images \u001b[39m=\u001b[39m pipe(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         prompt_ids \u001b[39mif\u001b[39;49;00m do_jit \u001b[39melse\u001b[39;49;00m prompt_ids[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         p_params \u001b[39mif\u001b[39;49;00m do_jit \u001b[39melse\u001b[39;49;00m params,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         rng \u001b[39mif\u001b[39;49;00m do_jit \u001b[39melse\u001b[39;49;00m rng[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         num_inference_steps\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39m# neg_prompt_ids=neg_prompt_ids if do_jit else neg_prompt_ids[0],\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         guidance_scale \u001b[39m=\u001b[39;49m \u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         jit\u001b[39m=\u001b[39;49mdo_jit,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     )\u001b[39m.\u001b[39mimages\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mreshape((images\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m images\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], ) \u001b[39m+\u001b[39m images\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B34.91.163.77/home/neverix/misgen/experiment_lcm.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pipe\u001b[39m.\u001b[39mnumpy_to_pil(np\u001b[39m.\u001b[39marray(images))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_flax_stable_diffusion_xl.py:115\u001b[0m, in \u001b[0;36mFlaxStableDiffusionXLPipeline.__call__\u001b[0;34m(self, prompt_ids, params, prng_seed, num_inference_steps, guidance_scale, height, width, latents, neg_prompt_ids, return_dict, output_type, jit)\u001b[0m\n\u001b[1;32m    112\u001b[0m return_latents \u001b[39m=\u001b[39m output_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlatent\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m jit:\n\u001b[0;32m--> 115\u001b[0m     images \u001b[39m=\u001b[39m _p_generate(\n\u001b[1;32m    116\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    117\u001b[0m         prompt_ids,\n\u001b[1;32m    118\u001b[0m         params,\n\u001b[1;32m    119\u001b[0m         prng_seed,\n\u001b[1;32m    120\u001b[0m         num_inference_steps,\n\u001b[1;32m    121\u001b[0m         height,\n\u001b[1;32m    122\u001b[0m         width,\n\u001b[1;32m    123\u001b[0m         guidance_scale,\n\u001b[1;32m    124\u001b[0m         latents,\n\u001b[1;32m    125\u001b[0m         neg_prompt_ids,\n\u001b[1;32m    126\u001b[0m         return_latents,\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[1;32m    130\u001b[0m         prompt_ids,\n\u001b[1;32m    131\u001b[0m         params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m         return_latents,\n\u001b[1;32m    140\u001b[0m     )\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/jax/_src/compiler.py:255\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mcompile(built_c, compile_options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    251\u001b[0m                          host_callbacks\u001b[39m=\u001b[39mhost_callbacks)\n\u001b[1;32m    252\u001b[0m \u001b[39m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcompile(built_c, compile_options\u001b[39m=\u001b[39;49moptions)\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 15.54G of 15.48G hbm. Exceeded hbm capacity by 57.28M.\n\nTotal hbm usage >= 16.06G:\n    reserved        530.00M \n    program           2.61G \n    arguments        12.93G \n\nOutput size 12.00M; shares 0B with arguments.\n\nProgram hbm requirement 2.61G:\n    global            83.0K\n    scoped           161.0K\n    HLO temp          2.54G (100.0% utilization: Unpadded (2.03G) Padded (2.03G), 20.1% fragmentation (522.18M))\n    overlays         65.82M\n\n  Largest program allocations in hbm:\n\n  1. Size: 524.00M\n     Shape: bf16[1024,8,131,256]{3,0,2,1:T(8,128)(2,1)}\n     Unpadded size: 524.00M\n     XLA label: fusion.85.remat.1 = fusion(slice.509, bitcast.2878, bitcast.7047), kind=kLoop, calls=fused_computation.73.clone.1\n     Allocation type: HLO temp\n     ==========================\n\n  2. Size: 524.00M\n     Shape: bf16[1024,8,131,256]{3,0,2,1:T(8,128)(2,1)}\n     Unpadded size: 524.00M\n     XLA label: fusion.85.remat.1 = fusion(slice.509, bitcast.2878, bitcast.7047), kind=kLoop, calls=fused_computation.73.clone.1\n     Allocation type: HLO temp\n     ==========================\n\n  3. Size: 516.00M\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv_shortcut/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: f32[1024,8,129,128]{3,1,2,0:T(8,128)}\n     Unpadded size: 516.00M\n     XLA label: fusion.253.remat = fusion(get-tuple-element.2519, fusion.95.remat, constant.1828, get-tuple-element.2518), kind=kOutput, calls=fused_computation.186.clone\n     Allocation type: HLO temp\n     ==========================\n\n  4. Size: 516.00M\n     Shape: bf16[1024,1,1032,256]{3,0,2,1:T(8,128)(2,1)}\n     Unpadded size: 516.00M\n     XLA label: fusion.18936 = fusion(copy.6580), kind=kLoop, calls=fused_computation.12772.clone\n     Allocation type: HLO temp\n     ==========================\n\n  5. Size: 65.82M\n     XLA label: overlays\n     Allocation type: overlays\n     ==========================\n\n  6. Size: 17.0K\n     Shape: pred[8,129]{0,1:T(8,128)(4,1)}\n     Unpadded size: 1.0K\n     Extra memory due to padding: 16.0K (16.9x expansion)\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  7. Size: 9.0K\n     Shape: pred[8,65]{0,1:T(8,128)(4,1)}\n     Unpadded size: 520B\n     Extra memory due to padding: 8.5K (17.7x expansion)\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  8. Size: 5.0K\n     Shape: pred[8,33]{0,1:T(8,128)(4,1)}\n     Unpadded size: 264B\n     Extra memory due to padding: 4.7K (19.4x expansion)\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  9. Size: 3.0K\n     Shape: pred[8,17]{0,1:T(8,128)(4,1)}\n     Unpadded size: 136B\n     Extra memory due to padding: 2.9K (22.6x expansion)\n     XLA label: constant literal\n     Allocation type: global\n     ==========================\n\n  10. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/while/body/FlaxUNet2DConditionModel/mid_block/attentions_0/transformer_blocks_2/attn2/to_out_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[2,1024]{1,0:T(2,128)}, f32[2,1024]{1,0:T(2,128)}, f32[2,1024,1280]{2,1,0:T(8,128)})\n     Unpadded size: 1.0K\n     XLA label: fusion.14368 = fusion(get-tuple-element.6536, get-tuple-element.3343, bitcast.1304, bitcast.9178), kind=kOutput, calls=fused_computation.12617\n     Allocation type: HLO temp\n     ==========================\n\n  11. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/while/body/FlaxUNet2DConditionModel/mid_block/attentions_0/transformer_blocks_2/attn2/b i d, b j d->b i j/dot_general[dimension_numbers=(((2,), (2,)), ((0,), (0,))) precision=None preferred_element_type=float32]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[40,1024]{1,0:T(8,128)}, f32[40,1024,77]{1,2,0:T(8,128)})\n     Unpadded size: 1.0K\n     XLA label: fusion.15566 = fusion(bitcast.1293, bitcast.1297), kind=kOutput, calls=fused_computation.12897\n     Allocation type: HLO temp\n     ==========================\n\n  12. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_2/norm1/div\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[32]{0:T(256)}, f32[32]{0:T(256)})\n     Unpadded size: 1.0K\n     XLA label: fusion.12878 = fusion(reduce.1300, reduce.1302), kind=kLoop, calls=fused_computation.11278\n     Allocation type: HLO temp\n     ==========================\n\n  13. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv2/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[128]{0:T(256)}, f32[128]{0:T(256)}, f32[1024,8,129,128]{3,1,2,0:T(8,128)})\n     Unpadded size: 1.0K\n     XLA label: fusion.267 = fusion(constant.1828, get-tuple-element.6881, get-tuple-element.2526, get-tuple-element.2527, ...(+3)), kind=kOutput, calls=fused_computation.196\n     Allocation type: HLO temp\n     ==========================\n\n  14. Size: 1.0K\n     Shape: (bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)}, bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)})\n     Unpadded size: 1.0K\n     XLA label: fusion.15362 = fusion(fusion.268), kind=kLoop, calls=fused_computation.12777\n     Allocation type: HLO temp\n     ==========================\n\n  15. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/norm2/div\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[32]{0:T(256)}, f32[32]{0:T(256)})\n     Unpadded size: 1.0K\n     XLA label: fusion.12879 = fusion(reduce.1297, reduce.1299), kind=kLoop, calls=fused_computation.11279\n     Allocation type: HLO temp\n     ==========================\n\n  16. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/conv1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[128]{0:T(256)}, f32[128]{0:T(256)}, f32[1024,8,129,128]{3,1,2,0:T(8,128)})\n     Unpadded size: 1.0K\n     XLA label: fusion.270 = fusion(constant.1828, get-tuple-element.2524, get-tuple-element.2525, get-tuple-element.5361, ...(+2)), kind=kOutput, calls=fused_computation.199\n     Allocation type: HLO temp\n     ==========================\n\n  17. Size: 1.0K\n     Shape: (bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)}, bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)})\n     Unpadded size: 1.0K\n     XLA label: fusion.15363 = fusion(fusion.271), kind=kLoop, calls=fused_computation.12778\n     Allocation type: HLO temp\n     ==========================\n\n  18. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_1/norm1/div\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[32]{0:T(256)}, f32[32]{0:T(256)})\n     Unpadded size: 1.0K\n     XLA label: fusion.12880 = fusion(reduce.1294, reduce.1296), kind=kLoop, calls=fused_computation.11280\n     Allocation type: HLO temp\n     ==========================\n\n  19. Size: 1.0K\n     Operator: op_name=\"pmap(_p_generate)/jit(main)/FlaxAutoencoderKL.decode/decoder/up_blocks_3/resnets_0/conv2/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_1003790/2774275608.py\" source_line=17\n     Shape: (f32[128]{0:T(256)}, f32[128]{0:T(256)}, f32[1024,8,129,128]{3,1,2,0:T(8,128)})\n     Unpadded size: 1.0K\n     XLA label: fusion.273 = fusion(constant.1828, fusion.253.remat, get-tuple-element.2516, get-tuple-element.2517, ...(+3)), kind=kOutput, calls=fused_computation.202\n     Allocation type: HLO temp\n     ==========================\n\n  20. Size: 1.0K\n     Shape: (bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)}, bf16[1024,1,7,1,128]{4,2,3,0,1:T(8,128)(2,1)})\n     Unpadded size: 1.0K\n     XLA label: fusion.15364 = fusion(fusion.274), kind=kLoop, calls=fused_computation.12779\n     Allocation type: HLO temp\n     ==========================\n\n"
     ]
    }
   ],
   "source": [
    "imgs_per_device = 1\n",
    "\n",
    "prompt = \"a cat\"\n",
    "prompts = [prompt] * jax.device_count() * imgs_per_device\n",
    "prompt_ids = pipe.prepare_inputs(prompts)\n",
    "\n",
    "prompt_ids = shard(prompt_ids)\n",
    "\n",
    "for u in generate(prompt_ids):\n",
    "    display(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
